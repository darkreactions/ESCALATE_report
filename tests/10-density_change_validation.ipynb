{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validating changes introduced in branch `10-density`\n",
    "\n",
    "    \n",
    "The version of `outputvalidation.py` at the time of writing returned `NARP` based on changes introduced in branch `10-density`. This notebook is a deep dive into why that happened. \n",
    "\n",
    "## Overview\n",
    "\n",
    "Branch `10-density` made the following changes: \n",
    "1. introduced new density variables to the report output\n",
    "    * This is the main change introduced in `10-density`\n",
    "2. solidified user's option to drop `_raw_*` features from output\n",
    "    * This secondary change impacts our work in this notebook\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A preliminary look at the data\n",
    "We observe that there are 368 columns in `old` dataset that *do not* appear in `new`, and three columns in `new` that do not appear in `old`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#old = pd.read_csv(\"../oldcomparetest.csv\") # data before the change\n",
    "#new = pd.read_csv(\"../debugtest.csv\")    # data after the change\n",
    "new.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculates the number of columns in the old dataset that do not appear in the new dataset\n",
    "np.isin(old.columns.values, new.columns.values, invert=True).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  calculates the number of columns in the new dataset that do not appear in the old datset\n",
    "np.isin(new.columns.values, old.columns.values, invert=True).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns missing from `new`\n",
    "\n",
    "Here we can take a closer look at the columns that appeared in `old` but not `new`.\n",
    "\n",
    "(Mike): We expected these to be the `_raw_*` columns, because of change 2. discussed in the **Overview**.\n",
    "However, observe below that some `_feat_*` columns appear in this set, I'm not sure why. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Ian): The `_feat_*` columns have been removed from the perovskite features due to being terrible fits for the current dataset.  The target dataset might need to be updated to reflect the new feature set at the conclusion of validating and pushing to master.  I will make this change and the `_feat_*` columns should no longer appear (unless of course there are expected changes to the reporting of features in the updates being tested in a new branch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_cols = list(set(old.columns.values) - set(new.columns.values))\n",
    "dropped_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dropped_feats = [col for col in dropped_cols if col[:4] != '_raw']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropped_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dropped_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dropped_feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(old.columns.values) - set(new.columns.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Columns added to `new`\n",
    "These are the new columns added to the report output by branch `10-density`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols = list(set(new.columns.values) - set(old.columns.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropping Mismatched Columns\n",
    "\n",
    "Once we drop the columns such that the two dfs have the set intersection of their columns, we expect the dataframes to be equal. However, thats not *quite* what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old.drop(dropped_cols, axis=1, inplace=True)\n",
    "old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.drop(list(new_cols), axis=1, inplace=True)\n",
    "new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "old.equals(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A different way of comparing all values of the dataframe\n",
    "(old == new).all().all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dealing with `NaN`s\n",
    "If we dig a little deeper into the differences between the two dataframes, it appears that `NaNs` are the culprit. \n",
    "\n",
    "If we just drop (few) rows that contain `NaNs`, everything looks good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gives us the indices of the rows and columns where mismatches are present \n",
    "\n",
    "mismatch_rows, mismatch_columns = list(map(lambda x: list(np.unique(x)), np.where(old != new)))\n",
    "mismatch_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old.iloc[mismatch_rows, mismatch_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new.iloc[mismatch_rows, mismatch_columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dropnans = old.drop(mismatch_rows)\n",
    "new_dropnans = new.drop(mismatch_rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that [pd.df.equals](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.equals.html) still returns `False`, but checking equality 'by hand' returns `True`. I am inclined to trust the 'by hand' check, since pandas can be quite finnickey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_dropnans.equals(new_dropnans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(old_dropnans == new_dropnans).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
